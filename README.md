A reference implementation for QoSBERT-style serviceâ€‘quality prediction built on the CodeGen languageâ€‘model backbone.  The project demonstrates how to transform userâ€“service metadata into semantic text, fineâ€‘tune a PLM with uncertainty estimation, and reproduce the results reported in our TSC submission â€œQoSBERT: An Uncertaintyâ€‘Aware Approach Based on Preâ€‘trained Language Models for Service Quality Prediction.â€

âœ¨Â Key Features

Component

Description

Semanticâ€¯templating

Converts numerical user/service metadata into naturalâ€‘language descriptions compatible with PLMs.

Monteâ€‘CarloÂ Dropout

Provides calibrated confidence intervals for each QoS prediction.

Multiâ€‘view pooling

MeanÂ +Â MaxÂ +Â Attention pooling over the topâ€‘K transformer layers to capture rich semantics.

Federatedâ€‘ready

Code structure keeps data loaders and model separately, making later migration to FL frameworks trivial.

ðŸ—‚ï¸Â Repository Layout
.
â”œâ”€â”€ dataset/                  # Preâ€‘processed WSâ€‘Dream splits
â”‚Â Â  â”œâ”€â”€ data_train_0.20_simplified.txt
â”‚Â Â  â”œâ”€â”€ data_test_0.20_simplified.txt
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ saved_models/             # Checkpoints & logs (created automatically)
â”œâ”€â”€ src/
â”‚Â Â  â”œâ”€â”€ run.py                # Main training / eval driver (HF Trainer wrapper)
â”‚Â Â  â”œâ”€â”€ data_utils.py         # JSONL â†’ tokenised Dataset objects
â”‚Â Â  â”œâ”€â”€ modeling_qos.py       # Gaussian head + MCâ€‘Dropout utilities
â”‚Â Â  â””â”€â”€ ...
â””â”€â”€ README.md                 # â† YouÂ areÂ here
ðŸ“¦Â Requirements

Package

Version tested

Python

3.10Â +

PyTorch

â‰¥Â 2.2.0 (CUDAÂ 11.8)

Transformers

â‰¥Â 4.40.0 (from source recommended)

Datasets

â‰¥Â 2.18.0

Accelerate

â‰¥Â 0.29.0
Quickâ€‘Start Training
# four GPUs (IDs 5,6,7,4) with gradientâ€‘accum 1 gives an effective batch of 1024
CUDA_VISIBLE_DEVICES=5,6,7,4 \
nohup python src/run.py \
  --output_dir ./saved_models/qos_codegen \
  --model_type codegen \
  --tokenizer_name /home/wangziliang/codegen \
  --model_name_or_path /home/wangziliang/codegen \
  --do_train --do_eval --do_test \
  --train_data_file dataset/data_train_0.20_simplified.txt \
  --eval_data_file  dataset/data_validation_0.20_simplified.txt \
  --test_data_file  dataset/data_test_0.20_simplified.txt \
  --epoch 20 \
  --block_size 64 \
  --train_batch_size 256 \
  --eval_batch_size 256 \
  --learning_rate 5e-5 \
  --max_grad_norm 1.0 \
  --evaluate_during_training \
  --seed 42 \
  > qos_train_logmlp20.txt 2>&1 &
> 

If you find this repo useful, please cite:

@article{wang2025qosbert,
  title     = {QoSBERT: An Uncertainty-Aware Approach Based on Pre-trained Language Models for Service Quality Prediction},
  author    = {Wang, Ziliang and Zhang, Xiaohong and Li, Ze Shi and Yan, Meng},
  journal   = {IEEE Transactions on Services Computing},
  year      = {2025},
  note      = {Under review}
}
